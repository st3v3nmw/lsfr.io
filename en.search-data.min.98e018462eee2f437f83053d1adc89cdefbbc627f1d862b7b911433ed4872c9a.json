[{"id":0,"href":"/guides/cli/","title":"CLI","section":"Guides","content":"CLI# Installation# go install# $ go install github.com/st3v3nmw/lsfr/cmd/lsfr@latest This installs lsfr to your $GOPATH/bin directory. Make sure it\u0026rsquo;s in your $PATH.\nUpdate# $ go install github.com/st3v3nmw/lsfr/cmd/lsfr@latest Verify Installation# $ lsfr list Available challenges: kv-store - Distributed Key-Value Store (8 stages) Start with: lsfr new \u0026lt;challenge-name\u0026gt; Quick Start# $ lsfr new kv-store # Create challenge in current directory $ lsfr test # Test your implementation $ lsfr next # Advance to next stage Edit run.sh to launch your implementation, then run lsfr test to get feedback.\nCommands support short aliases for faster typing:\nlsfr n â†’ lsfr new lsfr t â†’ lsfr test lsfr s â†’ lsfr status lsfr l or lsfr ls â†’ lsfr list Basic Workflow# 1. Start a Challenge# $ lsfr new \u0026lt;challenge\u0026gt; [path] Creates a new challenge directory with:\nrun.sh - Script that launches your implementation README.md - Challenge overview and requirements lsfr.yaml - Progress tracking .gitignore - Ignores .lsfr/ directory Examples:\n$ lsfr new kv-store # Create in current directory $ lsfr new kv-store my-kvs # Create in ./my-kvs 2. Implement and Test# Edit run.sh to start your implementation. The script must launch your server and pass through any arguments from lsfr:\n#!/bin/bash -e # Go exec go run ./cmd/server \u0026#34;$@\u0026#34; # Python # exec python main.py \u0026#34;$@\u0026#34; # Rust # cargo build --release \u0026amp;\u0026amp; exec ./target/release/kvstore \u0026#34;$@\u0026#34; # Node.js # exec node server.js \u0026#34;$@\u0026#34;Then test:\n$ lsfr test When tests pass:\nPASSED âœ“ Run \u0026#39;lsfr next\u0026#39; to advance to the next stage.When tests fail:\nFAILED âœ— PUT http://127.0.0.1:45123/kv/ \u0026#34;foo\u0026#34; Expected: \u0026#34;key cannot be empty\u0026#34; Actual: \u0026#34;\u0026#34; Your server accepted an empty key when it should reject it. Add validation to return 400 Bad Request for empty keys. Read the guide: lsfr.io/kv-store/http-apiFix the issues, then run lsfr test again. The CLI is designed for quick iteration, just keep running lsfr test as you make changes.\n3. Progress Through Stages# $ lsfr next Advances to the next stage after verifying the current stage passes. Updates lsfr.yaml automatically.\nIf the current stage hasn\u0026rsquo;t been completed, lsfr next runs tests first and only advances if they pass.\nWhen you complete a challenge, consider tagging your repo with lsfr-\u0026lt;language\u0026gt; (e.g., lsfr-go, lsfr-rust) to share your implementation.\nCommands Reference# lsfr new# Usage: lsfr new \u0026lt;challenge\u0026gt; [path]\nCreates a new challenge in the specified directory (or current directory if not specified).\nlsfr test# Usage: lsfr test [stage]\nRuns tests for the current stage (from lsfr.yaml) or a specific stage if provided.\n$ lsfr test # Test current stage $ lsfr test persistence # Test specific stage lsfr next# Usage: lsfr next\nAdvances to the next stage after verifying current stage passes all tests.\nlsfr status# Usage: lsfr status\nShows challenge progress and next steps:\n$ lsfr status Distributed Key-Value Store In this challenge, you\u0026#39;ll build a distributed key-value store from scratch. You\u0026#39;ll start with a single-node system that handles persistence and crash recovery, then implement Raft\u0026#39;s leader election, log replication, and fault tolerance mechanisms. Progress: âœ“ http-api - HTTP API with GET/PUT/DELETE Operations âœ“ persistence - Data Survives SIGTERM âœ“ crash-recovery - Data Survives SIGKILL â†’ leader-election - Raft Leader Election log-replication - Raft Log Replication membership-changes - Dynamic Cluster Membership fault-tolerance - Node Failures and Network Partitions log-compaction - Snapshots and Log Truncation Read the guide: lsfr.io/kv-store/leader-election Implement leader-election, then run \u0026#39;lsfr test\u0026#39;. lsfr list# Usage: lsfr list\nLists all available challenges with stage counts.\nUnderstanding lsfr.yaml# The config file tracks your progress:\nchallenge: kv-store stages: current: persistence completed: - http-apiYou can edit this manually to jump between stages or reset progress. You can also use lsfr test \u0026lt;stage\u0026gt; to test any stage without changing your current progress.\nCI/CD# Want to run tests automatically in your CI pipeline? See the CI/CD guide for GitHub Actions and other CI systems.\n"},{"id":1,"href":"/how-lsfr-works/","title":"How lsfr Works","section":"Home","content":"How lsfr Works# Reading about how systems work is one thing. Building them is another.\nlsfr breaks down complex systems into stages you can actually implement. Build a distributed database, write a compiler, or create a message queue from scratch. Each challenge starts simple and adds complexity one stage at a time.\nYou write real code that actually works. Tests verify your implementation handles the hard problems: network failures, crash recovery, concurrent access, etc. By the end, you understand these systems because you\u0026rsquo;ve built them yourself.\nChallenges# Each challenge breaks down a complex system into manageable stages that can be built incrementally. For instance, the distributed key-value store challenge starts with a simple in-memory store with a HTTP API and by the end, you have a sharded distributed key-value store.\nEach stage comes with tests that simulate real-world scenarios like network failures and crash recovery. The tests verify your system\u0026rsquo;s behavior, not implementation details, so you\u0026rsquo;re free to choose your own data structures, algorithms, and approaches. These are essentially end-to-end tests that check if your system actually works.\nEach stage explains what you need to implement and links out to good resources for learning more. Why reinvent the wheel when Kleppmann and others have already explained it better? ðŸ˜…\nReady to try it out? Here\u0026rsquo;s how to set up lsfr and start your first challenge.\nInstallation# See this guide on how to install lsfr on your system.\nPick a Challenge# Start with the distributed key-value store challenge; it\u0026rsquo;s a great introduction to distributed systems concepts. Check the challenge\u0026rsquo;s page for details on the first stage.\nScaffolding# Run lsfr new kv-store to create a new challenge directory with:\nrun.sh - Builds and runs your implementation README.md - Challenge overview and requirements lsfr.yaml - Tracks your progress Update run.sh with the commands to build and run your implementation. You can use any language - Go, Python, Rust, even Ponylang - as long as run.sh can start your program and pass through any command-line arguments from lsfr test.\nImplement \u0026amp; Test# Write your implementation in any language to solve the challenge\u0026rsquo;s first stage. When ready, run lsfr test to verify your solution works correctly. The tests focus on behavior, not implementation details.\nAdvance Through Stages# Pass the current stage, then run lsfr next to unlock the next stage. Each stage builds on the previous one, gradually adding complexity.\nBy the end of the challenge, you\u0026rsquo;ll have a deep understanding of how these systems actually work because you built one yourself.\nGood luck! ðŸš€\n"},{"id":2,"href":"/kv-store/http-api/","title":"HTTP API","section":"Distributed Key-Value Store","content":"HTTP API# In this stage, you\u0026rsquo;ll build an in-memory key-value store and expose it over a REST API.\nEndpoints# You\u0026rsquo;ll implement the following endpoints:\nPUT /kv/{key} Add or update a key-value pair in the store.\nPUT /kv/{key} Parameters: - key (path, required): The key to store (cannot be empty) Body: Value to store as plain text (cannot be empty) Response: - 200 OK: Key-value pair added or updated successfully - 400 Bad Request: Return \u0026#34;key cannot be empty\\n\u0026#34; or \u0026#34;value cannot be empty\\n\u0026#34; GET /kv/{key} Retrieve the value associated with the given key.\nGET /kv/{key} Parameters: - key (path, required): The key to retrieve Response: - 200 OK: Return the stored value - 404 Not Found: Return \u0026#34;key not found\\n\u0026#34; DELETE /kv/{key} Remove a key-value pair from the store.\nDELETE /kv/{key} Parameters: - key (path, required): The key to delete Response: - 200 OK: Key deleted successfully (or key didn\u0026#39;t exist) DELETE /clear Remove all key-value pairs from the store.\nDELETE /clear Response: - 200 OK: All keys cleared successfully Error Handling Unsupported HTTP methods on any endpoint should return:\n405 Method Not Allowed: Return \u0026ldquo;method not allowed\\n\u0026rdquo; Your API should handle concurrent requests safely. Consider thread safety when implementing your in-memory store.\nStorage# A simple in-memory map/dictionary is sufficient for storage in this stage. You\u0026rsquo;ll add persistence in the next stage.\nData Model# Keys and values are stored as simple strings. This keeps the data model straightforward so you can focus on building intuition in distributed systems, not implementing complex data types.\nKeys# Keys must be URL-safe strings without spaces or forward slashes. Examples of valid keys:\ncountry:capital user_123 special:key-with_symbols.123 Values# Values are stored as UTF-8 encoded text and can contain:\nUnicode characters like ðŸ˜Š Spaces and special symbols Long strings (up to reasonable memory limits) Testing# Your server must accept --port and --working-dir flags:\n$ ./run.sh --port 8080 --working-dir .lsfr/run-20251226-210357 --working-dir is your server\u0026rsquo;s working directory. Any files you create should go here, and your server\u0026rsquo;s logs will be in node.log.\nYou can test your implementation using the lsfr command:\n$ lsfr test http-api Testing http-api: HTTP API with GET/PUT/DELETE Operations âœ“ PUT Basic Operations âœ“ PUT Edge and Error Cases âœ“ GET Basic Operations âœ“ GET Edge and Error Cases âœ“ DELETE Basic Operations âœ“ DELETE Edge and Error Cases âœ“ CLEAR Operations âœ“ Concurrent Operations - Different Keys âœ“ Concurrent Operations - Same Key âœ“ Check Allowed HTTP Methods PASSED âœ“ Run \u0026#39;lsfr next\u0026#39; to advance to the next stage. Debugging# When tests fail, lsfr will show you exactly what went wrong:\n$ lsfr test Testing http-api: HTTP API with GET/PUT/DELETE Operations âœ“ PUT Basic Operations âœ“ PUT Edge and Error Cases âœ“ GET Basic Operations âœ— GET Edge and Error Cases GET http://127.0.0.1:42409/kv/nonexistent:key Expected response: \u0026#34;key not found\\n\u0026#34; Actual response: \u0026#34;\\n\u0026#34; Your server should return 404 Not Found when a key doesn\u0026#39;t exist. Check your key lookup logic and error handling. FAILED âœ— Read the guide: lsfr.io/kv-store/http-api You can also add your own logging to help debug. Your server\u0026rsquo;s output (stdout/stderr) is captured in node.log inside the working directory.\n"},{"id":3,"href":"/reference/testing/","title":"Testing","section":"Reference","content":"Testing Framework# The lsfr testing framework (attest) provides a fluent API for writing black-box tests against programs. Tests validate external behavior without accessing implementation internals.\nQuick Start# A typical test suite:\npackage kvstore import ( . \u0026#34;github.com/st3v3nmw/lsfr/internal/attest\u0026#34; ) func HTTPAPI() *Suite { return New(). // 0 Setup(func(do *Do) { do.Start(\u0026#34;node\u0026#34;) }). // 1 Test(\u0026#34;PUT stores data\u0026#34;, func(do *Do) { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key\u0026#34;, \u0026#34;value\u0026#34;). Returns().Status(Is(200)). Assert(\u0026#34;Your server should accept PUT requests.\\n\u0026#34; + \u0026#34;Ensure your HTTP handler processes PUT to /kv/{key}.\u0026#34;) }). // 2 Test(\u0026#34;GET retrieves data\u0026#34;, func(do *Do) { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Returns().Status(Is(200)).Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Your server should return stored values.\u0026#34;) }) }Tests run sequentially. State persists between tests so data written in test 1 is available in test 2. First failure stops execution.\nHTTP Assertions# Make HTTP requests and validate responses:\n// Basic request do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Returns().Status(Is(200)).Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Your server should return stored values.\u0026#34;) // With body do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key\u0026#34;, \u0026#34;value\u0026#34;). Returns().Status(Is(200)). Assert(\u0026#34;Your server should accept PUT requests.\u0026#34;) // With custom headers do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;/api\u0026#34;, `{\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;}`, H{\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}). Returns().Status(Is(201)). Assert(...) // Status only do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;/kv/key\u0026#34;). Returns().Status(Is(200)). Assert(\u0026#34;Your server should accept DELETE requests.\u0026#34;)CLI Assertions# Execute CLI commands and validate output:\n// Check exit code and output do.Exec(\u0026#34;--help\u0026#34;). Returns().ExitCode(Is(0)).Output(Contains(\u0026#34;Usage:\u0026#34;)). Assert(\u0026#34;Your command should show usage information.\u0026#34;) // Check exit code only do.Exec(\u0026#34;invalid\u0026#34;, \u0026#34;args\u0026#34;). Returns().ExitCode(Is(1)). Assert(\u0026#34;Your command should reject invalid arguments.\u0026#34;)Matchers# Is(value)# Exact equality:\n.Status(Is(200)) .Body(Is(\u0026#34;key not found\\n\u0026#34;)) .ExitCode(Is(0))Contains(substring)# String contains:\n.Body(Contains(\u0026#34;error\u0026#34;)) .Output(Contains(\u0026#34;Usage:\u0026#34;))Matches(pattern)# Regex matching:\n.Body(Matches(`^[0-9]+$`)) .Output(Matches(`version \\d+\\.\\d+\\.\\d+`))OneOf(values\u0026hellip;)# Match any of several values:\n// Useful for concurrent operations where order is non-deterministic .Body(OneOf(\u0026#34;value1\u0026#34;, \u0026#34;value2\u0026#34;, \u0026#34;value3\u0026#34;))Not(matcher)# Negates another matcher:\n.Status(Not(Is(500))) .Body(Not(Contains(\u0026#34;panic\u0026#34;)))Timing# Eventually()# Retry until condition becomes true or timeout (default 5s):\n// Wait for replica to sync do.HTTP(\u0026#34;replica\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Eventually(). Returns().Status(Is(200)).Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Replica should eventually receive replicated data.\u0026#34;) // Custom timeout do.HTTP(\u0026#34;replica\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Eventually().Within(10 * time.Second). Returns().Status(Is(200)).Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Replica should sync within 10 seconds.\u0026#34;)Consistently()# Verify condition stays true for duration (default 5s):\n// Verify value remains stable do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Consistently(). Returns().Status(Is(200)).Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Value should remain stable.\u0026#34;) // Custom duration do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Consistently().For(2 * time.Second). Returns().Status(Is(200)).Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Value should remain stable for 2 seconds.\u0026#34;)Default (Immediate)# Without Eventually() or Consistently(), checks execute once immediately:\ndo.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Returns().Status(Is(200)). Assert(\u0026#34;Your server should return the value immediately.\u0026#34;)Service Management# Start(name, args\u0026hellip;)# Start a service with auto-assigned port:\ndo.Start(\u0026#34;node\u0026#34;) do.Start(\u0026#34;replica\u0026#34;, \u0026#34;--seed=123\u0026#34;)Port is auto-assigned by the OS. Services receive --port and --working-dir flags automatically. Each test run gets an isolated working directory like run-20240115-143022.\nStop(name)# Graceful shutdown with SIGTERM:\ndo.Stop(\u0026#34;node\u0026#34;)Sends SIGTERM and waits for graceful exit. If process doesn\u0026rsquo;t exit within timeout, sends SIGKILL.\nKill(name)# Immediate termination with SIGKILL:\ndo.Kill(\u0026#34;node\u0026#34;)Restart(name, sig\u0026hellip;)# Restart a service:\n// Graceful restart (SIGTERM) do.Restart(\u0026#34;node\u0026#34;) // Crash simulation (SIGKILL) do.Restart(\u0026#34;node\u0026#34;, syscall.SIGKILL)The optional signal parameter controls how the process is stopped before restart. SIGKILL simulates crashes with no cleanup, SIGTERM allows graceful shutdown.\nConcurrency# Concurrently(funcs\u0026hellip;)# Run operations in parallel:\ndo.Concurrently( func() { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key1\u0026#34;, \u0026#34;value1\u0026#34;). Returns().Status(Is(200)). Assert(...) }, func() { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key2\u0026#34;, \u0026#34;value2\u0026#34;). Returns().Status(Is(200)). Assert(...) }, ) // Verify both succeeded do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key1\u0026#34;). Returns().Status(Is(200)).Body(Is(\u0026#34;value1\u0026#34;)). Assert(...) do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key2\u0026#34;). Returns().Status(Is(200)).Body(Is(\u0026#34;value2\u0026#34;)). Assert(...)Waits for all functions to complete. If any panic, the first panic is re-raised after all complete.\nWriting Good Assertions# Assertion messages appear when tests fail. They should help developers fix the problem.\nGood assertion messages describe expected behavior, provide concrete next steps, and reference relevant concepts:\nAssert(\u0026#34;Your server should reject empty keys.\\n\u0026#34; + \u0026#34;Add validation to return 400 Bad Request for empty keys.\u0026#34;) Assert(\u0026#34;Your server should preserve data across crashes.\\n\u0026#34; + \u0026#34;Implement a Write-Ahead Log (WAL) that records operations before applying them.\\n\u0026#34; + \u0026#34;Ensure writes are durably stored before acknowledging to the client.\u0026#34;)Focus on requirements rather than implementation details. Say \u0026ldquo;Ensure writes are durably stored before acknowledging\u0026rdquo; instead of \u0026ldquo;You must call fsync\u0026rdquo;.\nAvoid:\nGeneric messages: \u0026ldquo;Fix your code\u0026rdquo; Vague messages: \u0026ldquo;This is wrong\u0026rdquo; Past tense: \u0026ldquo;Your server accepted an empty key when it should reject it\u0026rdquo; Unnecessary adverbs: \u0026ldquo;Your server should handle requests correctly\u0026rdquo; Example error output:\nPUT /kv/ \u0026#34;value\u0026#34; Expected response: \u0026#34;key cannot be empty\\n\u0026#34; Actual response: \u0026#34;\u0026#34; Your server should reject empty keys. Add validation to return 400 Bad Request for empty keys. Creating a Challenge# Directory Structure# challenges/ â””â”€â”€ kvstore/ â”œâ”€â”€ init.go # Challenge registration â”œâ”€â”€ http_api.go # Stage 1 â”œâ”€â”€ persistence.go # Stage 2 â””â”€â”€ crash_recovery.go # Stage 3Stage Structure# Each stage is a function returning *Suite:\npackage kvstore import ( . \u0026#34;github.com/st3v3nmw/lsfr/internal/attest\u0026#34; ) func HTTPAPI() *Suite { return New(). // 0 Setup(func(do *Do) { do.Start(\u0026#34;node\u0026#34;) }). // 1 Test(\u0026#34;PUT Basic Operations\u0026#34;, func(do *Do) { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key\u0026#34;, \u0026#34;value\u0026#34;). Returns().Status(Is(200)). Assert(\u0026#34;Your server should accept PUT requests.\u0026#34;) }). // 2 Test(\u0026#34;GET Basic Operations\u0026#34;, func(do *Do) { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Returns().Status(Is(200)).Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Your server should return stored values.\u0026#34;) }) }Import attest with . for cleaner syntax. Number tests with comments (0, 1, 2\u0026hellip;) to visually separate tests so that they\u0026rsquo;re not crowded.\nChallenge Registration# Create init.go:\npackage kvstore import \u0026#34;github.com/st3v3nmw/lsfr/internal/registry\u0026#34; func init() { challenge := \u0026amp;registry.Challenge{ Name: \u0026#34;Distributed Key-Value Store\u0026#34;, Summary: `Build a distributed key-value store from scratch. You\u0026#39;ll start with a simple HTTP API and progressively add persistence, crash recovery, clustering, replication, and consensus.`, } challenge.AddStage(\u0026#34;http-api\u0026#34;, \u0026#34;HTTP API with GET/PUT/DELETE Operations\u0026#34;, HTTPAPI) challenge.AddStage(\u0026#34;persistence\u0026#34;, \u0026#34;Data Survives SIGTERM\u0026#34;, Persistence) challenge.AddStage(\u0026#34;crash-recovery\u0026#34;, \u0026#34;Crash Recovery with Write-Ahead Logging\u0026#34;, CrashRecovery) registry.RegisterChallenge(\u0026#34;kv-store\u0026#34;, challenge) }Auto-Discovery# Import your challenge in challenges/challenges.go:\npackage challenges import ( _ \u0026#34;github.com/st3v3nmw/lsfr/challenges/kvstore\u0026#34; )"},{"id":4,"href":"/guides/ci-cd/","title":"CI/CD","section":"Guides","content":"CI/CD# Run lsfr tests automatically in GitHub Actions.\nGitHub Actions# Add .github/workflows/lsfr.yaml to your repository:\nname: lsfr Tests on: push: branches: [main] pull_request: branches: [main] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v6 - uses: st3v3nmw/lsfr-action@mainThe action runs lsfr test on every push to main and on pull requests.\nCustom Working Directory# If your lsfr.yaml isn\u0026rsquo;t at the repository root:\n- uses: st3v3nmw/lsfr-action@main with: working-directory: \u0026#39;./my-challenge\u0026#39;"},{"id":5,"href":"/kv-store/","title":"Distributed Key-Value Store","section":"Home","content":"Distributed Key-Value Store Challenge# Welcome to the distributed key-value store challenge!\nIn this challenge, you\u0026rsquo;ll build a distributed key-value store from scratch. You\u0026rsquo;ll start with a single-node system that handles persistence and crash recovery, then implement Raft\u0026rsquo;s leader election, log replication, and fault tolerance mechanisms.\nBy the end, you\u0026rsquo;ll have built a production-grade distributed system that maintains strong consistency guarantees even during node failures and network partitions.\nStages# HTTP API Build a basic in-memory key-value store with GET/PUT/DELETE operations over HTTP.\nPersistence Add persistence to your store. Data should survive clean shutdowns (SIGTERM).\nCrash Recovery Ensure data consistency after crashes. Data should survive unclean shutdowns (SIGKILL).\nLeader Election Form a cluster and elect a leader using the Raft consensus algorithm.\nLog Replication Replicate operations from the leader to followers with strong consistency guarantees.\nMembership Changes Dynamically add and remove nodes from the cluster without downtime.\nFault Tolerance Handle node failures and network partitions while maintaining safety guarantees.\nLog Compaction Prevent unbounded log growth through snapshots and log truncation.\nGetting Started# If you haven\u0026rsquo;t already, read this overview on how lsfr works and then start with stage 1 (HTTP API).\nResources# Books# Designing Data-Intensive Applications by Martin Kleppmann Database Internals by Alex Petrov Papers# Raft Paper by Diego Ongaro \u0026amp; John Ousterhout Videos# Distributed Systems lecture series by Martin Kleppmann Implementations# little-key-value in Go by @st3v3nmw "},{"id":6,"href":"/kv-store/persistence/","title":"Persistence","section":"Distributed Key-Value Store","content":"Persistence# In this stage, you\u0026rsquo;ll add persistence to your key-value store. Data should survive clean shutdowns and be restored when the server restarts.\nGraceful Shutdown# When your server receives a SIGTERM signal, it should:\nWait for in-flight requests to complete (within 5 seconds) Save all key-value pairs to disk Exit with status code 0 Startup Recovery# When your server starts, it should:\nCheck the working directory for existing data Load any previously saved key-value pairs Continue serving requests with the restored data If no previous data exists, start with an empty store.\nStorage# Save your in-memory state to disk during shutdown and restore it on startup. Create your data files in the working directory (passed via --working-dir). The serialization format and file naming are up to you - JSON, binary, plain text, whatever.\nThis approach survives clean shutdowns but not crashes. If the process dies unexpectedly, you\u0026rsquo;ll lose any data that wasn\u0026rsquo;t saved. That\u0026rsquo;s fine for this stage - you\u0026rsquo;ll add crash recovery in the next stage.\nTesting# Your server will be started with the working directory where it should store data:\n$ ./run.sh --port 8080 --working-dir .lsfr/run-20251226-210357 You can test your implementation using the lsfr command:\n$ lsfr test Testing persistence: Data Survives SIGTERM âœ“ Verify Data Survives Graceful Restart âœ“ Check Data Integrity After Multiple Restarts âœ“ Test Persistence When Under Concurrent Load PASSED âœ“ Run \u0026#39;lsfr next\u0026#39; to advance to the next stage. The test will:\nStore data in your server Send SIGTERM to trigger graceful shutdown Restart your server Verify all data is still present "},{"id":7,"href":"/reference/writing-style/","title":"Writing Style","section":"Reference","content":"Writing Style# Audience# Write for capable peers: mid-level+ developers who can research concepts, make implementation choices, and work on complex problems.\nState what the system needs to accomplish, not how to build it. Point to possible approaches or considerations, but let readers make implementation choices.\nPedagogy# Structure learning through progressive disclosure. Start each challenge with the simplest version of a problem, then add complexity only after the foundation is solid. Make each stage build on previous work so the progression feels natural rather than arbitrary.\nIntroduce one major capability per stage: one testable system behavior that builds on previous work. Supporting concepts can appear as needed, but each stage should add one clear building block to the system.\nConstrain interfaces and contracts that affect testing or compatibility. Leave internal implementation details (data structures, algorithms, optimizations) to the reader\u0026rsquo;s judgment.\nLink to external resources when you introduce concepts. Point to tutorials, papers, lectures, or reference implementations - whatever explains it best. Don\u0026rsquo;t replicate explanations that already exist; point to the best resource and move on.\nVoice \u0026amp; Tone# Address the reader directly using second person (\u0026ldquo;you\u0026rdquo;) and active voice. Write as if you\u0026rsquo;re giving clear directions to a colleague, not lecturing from a podium. Use simple, everyday language.\nGet to the point immediately. Skip introductory context about why topics are important - your reader already has that context. Brief section transitions are fine, but avoid elaborate setup. Do explain the reasoning behind specific constraints or design choices you\u0026rsquo;re imposing.\nMaintain a matter-of-fact tone. Skip reassurance and cheerleading. An occasional emoji or light moment is fine, but default to straightforward instruction.\nFormatting# Structure each stage with a brief introduction (one or two sentences on what the reader will build), followed by precise specifications, implementation guidance, and testing instructions. Optionally include a debugging section with example test failures or debugging techniques. When showing failures, provide actionable guidance: expected versus actual output, then what to check or what might have gone wrong.\nWhen defining API contracts, data formats, or expected behaviors, be exact. Include complete endpoint specifications with methods, parameters, responses, and literal error messages. Specify data constraints explicitly. The goal is removing ambiguity about what to build without prescribing how to build it.\nUse code blocks to show test invocations, command-line usage, and expected outputs - not implementation code or algorithms. Readers should see how to verify their work and what correct behavior looks like. When test behavior isn\u0026rsquo;t obvious from output alone, briefly explain what the test does, especially when it affects implementation constraints (which signals, timing requirements, error conditions to handle).\nReserve callouts for critical non-obvious concerns. Don\u0026rsquo;t use them for general information that belongs in body text.\nLink concepts inline when they first appear. Add comprehensive resources like books, lecture series, \u0026amp; reference implementations to the challenge\u0026rsquo;s index page.\n"},{"id":8,"href":"/kv-store/crash-recovery/","title":"Crash Recovery","section":"Distributed Key-Value Store","content":"Crash Recovery# Your server currently saves data on clean shutdown but loses everything if it crashes. In this stage, you\u0026rsquo;ll add durability so data survives unexpected failures.\nWrite-Ahead Logging# Implement a Write-Ahead Log (WAL) that records operations before they\u0026rsquo;re applied to memory. Each write operation must be written to the log file before updating your in-memory store.\nLog Format# Your log should record operations in append-only fashion. The format is up to you - JSONL (one JSON object per line), binary serialization, or plain text all work.\nEach log entry needs enough information to replay the operation:\nOperation type (e.g., \u0026ldquo;set\u0026rdquo;, \u0026ldquo;delete\u0026rdquo;, \u0026ldquo;clear\u0026rdquo;) Key Value Any other metadata you need for replay Durability# After appending an operation to the log, ensure it\u0026rsquo;s physically written to disk before responding to the client. Use your language\u0026rsquo;s file sync mechanism (fsync, flush, etc.) to force the operating system to persist the write.\nWithout sync, the OS may buffer writes in memory and you\u0026rsquo;ll lose data on crash.\nSyncing on every operation is slow since you\u0026rsquo;re forcing a disk write and blocking the response. This is the correct trade-off for durability, but it limits throughput. Production databases use techniques like batching to amortize the fsync cost across multiple operations.\nRecovery Procedure# When your server starts:\nLoad the most recent snapshot (from the persistence stage) if one exists Replay all operations from the WAL that occurred after the snapshot Resume serving requests If no snapshot exists, replay the entire log from the beginning.\nHandling Corrupted Logs# The log file may contain partial writes at the end if the server crashed mid-write. Your replay logic should handle this gracefully:\nSkip incomplete/corrupted entries at the end of the log Process all valid entries before the corruption Continue serving requests with the recovered data Checkpointing# As your log grows, replaying from the beginning becomes slow. Periodically create snapshots of your in-memory state and truncate the log.\nWhen to checkpoint is up to you - after N operations, every M seconds, when the log reaches a certain size, etc. The test doesn\u0026rsquo;t care about your checkpoint strategy, only that recovery works correctly.\nAfter creating a snapshot:\nWrite the snapshot to a new file Truncate or create a new WAL file Continue logging operations On recovery, load the latest snapshot and replay only the operations logged after that snapshot.\nStorage Layout# You now have two types of files:\nSnapshot: Full state at a point in time (from previous stage) WAL: Operations logged since the last snapshot Organize these in the working directory however makes sense - separate files, subdirectories, naming conventions, etc. The test only cares that recovery works, not how you structure the files.\nTesting# Your server will be started with the working directory:\n$ ./run.sh --port 8080 --working-dir .lsfr/run-20251226-210357 Your server will be tested with unexpected crashes:\n$ lsfr test crash-recovery Testing crash-recovery: Data Survives SIGKILL âœ“ Basic WAL Durability âœ“ Multiple Crash Recovery Cycles âœ“ Rapid Write Burst Before Crash âœ“ Test Recovery When Under Concurrent Load PASSED âœ“ Run \u0026#39;lsfr next\u0026#39; to advance to the next stage. The test will:\nStore data in your server Kill the server process (SIGKILL) without warning Restart your server Verify all data that was acknowledged before the crash is still present "},{"id":9,"href":"/kv-store/leader-election/","title":"Leader Election","section":"Distributed Key-Value Store","content":"Leader Election# // TODO\nRough Notes# Form static 3-node cluster (\u0026ndash;peers flag with comma-separated addresses, or API) Implement Raft leader election (Section 5.2 of Raft paper) Server states: follower, candidate, leader Terms as logical clock RequestVote RPC Election timeouts (randomized 150-300ms) Leader heartbeats (AppendEntries RPC, empty for now) Split vote handling Only leader accepts client requests, followers return 503 Persist currentTerm and votedFor to survive crashes "},{"id":10,"href":"/kv-store/log-replication/","title":"Log Replication","section":"Distributed Key-Value Store","content":"Log Replication# // TODO\nRough Notes# Raft log replaces the WAL from stage 3 Leader appends client operations to log, replicates to followers AppendEntries RPC with actual log entries Commit index (entry committed when replicated to majority) Apply committed entries to state machine (KV store) Log matching property (consistency across nodes) Fsync log entries before responding to client nextIndex and matchIndex tracking per follower Happy path only - no crashes/failures yet "},{"id":11,"href":"/kv-store/membership-changes/","title":"Membership Changes","section":"Distributed Key-Value Store","content":"Membership Changes# // TODO\nRough Notes# Dynamically add/remove nodes from cluster Replicate configuration changes through Raft log Joint consensus (C_old,new) to prevent split brain during transitions Or single-server changes (simpler, safer) AddServer and RemoveServer operations Catch up new servers before adding to cluster Handle leader stepping down if removed from cluster "},{"id":12,"href":"/kv-store/fault-tolerance/","title":"Fault Tolerance","section":"Distributed Key-Value Store","content":"Fault Tolerance# // TODO\nRough Notes# Handle leader crashes (followers detect timeout, elect new leader) Handle follower crashes (leader retries AppendEntries) Log repair on recovery (conflicting entries, missing entries) Network partitions (majority partition continues, minority blocks) Safety: committed entries never lost Log consistency checks (prevLogIndex, prevLogTerm) Truncate conflicting uncommitted entries Leader completeness property "},{"id":13,"href":"/kv-store/log-compaction/","title":"Log Compaction","section":"Distributed Key-Value Store","content":"Log Compaction# // TODO\nRough Notes# Prevent unbounded log growth Take snapshots of state machine (KV store) Truncate log after snapshot InstallSnapshot RPC for slow/catching-up followers Snapshot metadata (last included index, last included term) Incremental snapshots or full state dump When to snapshot (configurable: every N entries, every M seconds, size threshold) Note: snapshot size = data size problem (document limitation, point to storage engine solutions) "},{"id":14,"href":"/about/","title":"About","section":"Home","content":"About# lsfr /ËˆÉ›l ËˆÉ›s ËˆÉ›f ËˆÉ‘r/ noun\nNamed after the song \u0026ldquo;Love Songs For Robots\u0026rdquo; by Patrick Watson.\nI have always been interested in distributed systems so when I finally got around to reading Designing Data-Intensive Applications, I thought \u0026ldquo;you know what, I should actually build some of this stuff to really understand it\u0026rdquo;. And that\u0026rsquo;s how lsfr was born: it breaks down complex systems into stages you can implement and test.\nBuilt by Stephen Mwangi.\n"}]