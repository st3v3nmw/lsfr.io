[{"id":0,"href":"/guides/cli/","title":"CLI","section":"Guides","content":"CLI# Installation# go install# $ go install github.com/st3v3nmw/lsfr/cmd/lsfr@latest This installs lsfr to your $GOPATH/bin directory. Make sure it\u0026rsquo;s in your $PATH.\nUpdate# $ go install github.com/st3v3nmw/lsfr/cmd/lsfr@latest Verify Installation# $ lsfr list Available challenges: kv-store - Distributed Key-Value Store (8 stages) Start with: lsfr new \u0026lt;challenge-name\u0026gt; Quick Start# $ lsfr new kv-store # Create challenge in current directory $ lsfr test # Test your implementation $ lsfr next # Advance to next stage Edit run.sh to launch your implementation, then run lsfr test to get feedback.\nCommands support short aliases for faster typing:\nlsfr n â†’ lsfr new lsfr t â†’ lsfr test lsfr s â†’ lsfr status lsfr l or lsfr ls â†’ lsfr list Basic Workflow# 1. Start a Challenge# $ lsfr new \u0026lt;challenge\u0026gt; [path] Creates a new challenge directory with:\nrun.sh - Script that launches your implementation README.md - Challenge overview and requirements lsfr.yaml - Progress tracking .gitignore - Ignores .lsfr/ directory Examples:\n$ lsfr new kv-store # Create in current directory $ lsfr new kv-store my-kvs # Create in ./my-kvs 2. Implement \u0026amp; Test# Edit run.sh to start your implementation. The script must launch your server and pass through any arguments from lsfr:\n#!/bin/bash -e # Go exec go run ./cmd/server \u0026#34;$@\u0026#34; # Python # exec python main.py \u0026#34;$@\u0026#34; # Rust # cargo build --release \u0026amp;\u0026amp; exec ./target/release/kvstore \u0026#34;$@\u0026#34; # Node.js # exec node server.js \u0026#34;$@\u0026#34;Then test:\n$ lsfr test When tests pass:\nPASSED âœ“ Run \u0026#39;lsfr next\u0026#39; to advance to the next stage.When tests fail:\nFAILED âœ— PUT http://127.0.0.1:45123/kv/ \u0026#34;foo\u0026#34; Expected: \u0026#34;key cannot be empty\u0026#34; Actual: \u0026#34;\u0026#34; Your server accepted an empty key when it should reject it. Add validation to return 400 Bad Request for empty keys. Read the guide: lsfr.io/kv-store/http-apiFix the issues, then run lsfr test again. The CLI is designed for quick iteration, just keep running lsfr test as you make changes.\n3. Progress Through Stages# $ lsfr next Advances to the next stage after verifying the current stage passes. Updates lsfr.yaml automatically.\nIf the current stage hasn\u0026rsquo;t been completed, lsfr next runs tests first and only advances if they pass.\nWhen you complete a challenge, consider tagging your repo with lsfr-\u0026lt;language\u0026gt; (e.g., lsfr-go, lsfr-rust) to share your implementation.\nCommands Reference# lsfr new# Usage: lsfr new \u0026lt;challenge\u0026gt; [path]\nCreates a new challenge in the specified directory (or current directory if not specified).\nlsfr test# Usage: lsfr test [stage]\nRuns tests for the current stage (from lsfr.yaml) or a specific stage if provided.\n$ lsfr test # Test current stage $ lsfr test persistence # Test specific stage lsfr next# Usage: lsfr next\nAdvances to the next stage after verifying current stage passes all tests.\nlsfr status# Usage: lsfr status\nShows challenge progress and next steps:\n$ lsfr status Distributed Key-Value Store In this challenge, you\u0026#39;ll build a distributed key-value store from scratch. You\u0026#39;ll start with a single-node system that handles persistence and crash recovery, then implement Raft\u0026#39;s leader election, log replication, and fault tolerance mechanisms. Progress: âœ“ http-api - Store and Retrieve Data âœ“ persistence - Data Survives SIGTERM âœ“ crash-recovery - Data Survives SIGKILL â†’ leader-election - Cluster Elects and Maintains Leader log-replication - Data Replicates to All Nodes membership-changes - Add and Remove Nodes Dynamically fault-tolerance - Cluster Survives Failures and Partitions log-compaction - System Manages Log Growth Read the guide: lsfr.io/kv-store/leader-election Implement leader-election, then run \u0026#39;lsfr test\u0026#39;. lsfr list# Usage: lsfr list\nLists all available challenges with stage counts.\nUnderstanding lsfr.yaml# The config file tracks your progress:\nchallenge: kv-store stages: current: persistence completed: - http-apiYou can edit this manually to jump between stages or reset progress. You can also use lsfr test \u0026lt;stage\u0026gt; to test any stage without changing your current progress.\nCI/CD# Want to run tests automatically in your CI pipeline? See the CI/CD guide for GitHub Actions and other CI systems.\n"},{"id":1,"href":"/how-lsfr-works/","title":"How lsfr Works","section":"Home","content":"How lsfr Works# Reading about how systems work is one thing. Building them is another.\nlsfr breaks down complex systems into stages you can actually implement. Build a distributed database, write a compiler, or create a message queue from scratch. Each challenge starts simple and adds complexity one stage at a time.\nYou write real code that actually works. Tests verify your implementation handles the hard problems: network failures, crash recovery, concurrent access, etc. By the end, you understand these systems because you\u0026rsquo;ve built them yourself.\nChallenges# Each challenge breaks down a complex system into manageable stages that can be built incrementally. For instance, the distributed key-value store challenge starts with a simple in-memory store with a HTTP API and by the end, you have a sharded distributed key-value store.\nEach stage comes with tests that simulate real-world scenarios like network failures and crash recovery. The tests verify your system\u0026rsquo;s behavior, not implementation details, so you\u0026rsquo;re free to choose your own data structures, algorithms, and approaches. These are essentially end-to-end tests that check if your system actually works.\nEach stage explains what you need to implement and links out to good resources for learning more. Why reinvent the wheel when Kleppmann and others have already explained it better? ðŸ˜…\nReady to try it out? Here\u0026rsquo;s how to set up lsfr and start your first challenge.\nInstallation# See this guide on how to install lsfr on your system.\nPick a Challenge# Start with the distributed key-value store challenge; it\u0026rsquo;s a great introduction to distributed systems concepts. Check the challenge\u0026rsquo;s page for details on the first stage.\nScaffolding# Run lsfr new kv-store to create a new challenge directory with:\nrun.sh - Builds and runs your implementation README.md - Challenge overview and requirements lsfr.yaml - Tracks your progress Update run.sh with the commands to build and run your implementation. You can use any language - Go, Python, Rust, even Ponylang - as long as run.sh can start your program and pass through any command-line arguments from lsfr test.\nImplement \u0026amp; Test# Write your implementation in any language to solve the challenge\u0026rsquo;s first stage. When ready, run lsfr test to verify your solution works correctly. The tests focus on behavior, not implementation details.\nAdvance Through Stages# Pass the current stage, then run lsfr next to unlock the next stage. Each stage builds on the previous one, gradually adding complexity.\nBy the end of the challenge, you\u0026rsquo;ll have a deep understanding of how these systems actually work because you built one yourself.\nGood luck! ðŸš€\n"},{"id":2,"href":"/kv-store/http-api/","title":"HTTP API","section":"Distributed Key-Value Store","content":"HTTP API# In this stage, you\u0026rsquo;ll build an in-memory key-value store and expose it over a REST API.\nEndpoints# You\u0026rsquo;ll implement the following endpoints:\nPUT /kv/{key} Add or update a key-value pair in the store.\nPUT /kv/{key} Parameters: - key (path, required): The key to store (cannot be empty) Body: Value to store as plain text (cannot be empty) Response: - 200 OK: Key-value pair added or updated successfully - 400 Bad Request: Return \u0026#34;key cannot be empty\\n\u0026#34; or \u0026#34;value cannot be empty\\n\u0026#34; GET /kv/{key} Retrieve the value associated with the given key.\nGET /kv/{key} Parameters: - key (path, required): The key to retrieve Response: - 200 OK: Return the stored value - 404 Not Found: Return \u0026#34;key not found\\n\u0026#34; DELETE /kv/{key} Remove a key-value pair from the store.\nDELETE /kv/{key} Parameters: - key (path, required): The key to delete Response: - 200 OK: Key deleted successfully (or key didn\u0026#39;t exist) DELETE /clear Remove all key-value pairs from the store.\nDELETE /clear Response: - 200 OK: All keys cleared successfully Error Handling Unsupported HTTP methods on any endpoint should return:\n405 Method Not Allowed: Return \u0026ldquo;method not allowed\\n\u0026rdquo; Your API should handle concurrent requests safely. Consider thread safety when implementing your in-memory store.\nStorage# A simple in-memory map/dictionary is sufficient for storage in this stage. You\u0026rsquo;ll add persistence in the next stage.\nData Model# Keys and values are stored as simple strings. This keeps the data model straightforward so you can focus on building intuition in distributed systems, not implementing complex data types.\nKeys# Keys must contain only alphanumeric plus :, _, ., \u0026amp; - characters. Examples of valid keys:\ncountry:capital user_123 special:key-with_symbols.123 Values# Values are stored as UTF-8 encoded text and can contain:\nUnicode characters like ðŸ˜Š Spaces and special symbols Long strings (up to reasonable memory limits) Testing# Your server must accept --port and --working-dir flags:\n$ ./run.sh --port 8001 --working-dir .lsfr/run-20251226-210357 --working-dir is your server\u0026rsquo;s working directory. Any files you create should go here, and your server\u0026rsquo;s logs will be in node.log.\nYou can test your implementation using the lsfr command:\n$ lsfr test http-api Testing http-api: Store and Retrieve Data âœ“ PUT Basic Operations âœ“ PUT Edge and Error Cases âœ“ GET Basic Operations âœ“ GET Edge and Error Cases âœ“ DELETE Basic Operations âœ“ DELETE Edge and Error Cases âœ“ CLEAR Operations âœ“ Concurrent Operations - Different Keys âœ“ Concurrent Operations - Same Key âœ“ Check Allowed HTTP Methods PASSED âœ“ Run \u0026#39;lsfr next\u0026#39; to advance to the next stage. Debugging# When tests fail, lsfr will show you exactly what went wrong:\n$ lsfr test Testing http-api: Store and Retrieve Data âœ“ PUT Basic Operations âœ“ PUT Edge and Error Cases âœ“ GET Basic Operations âœ— GET Edge and Error Cases GET http://127.0.0.1:42409/kv/nonexistent:key Expected response: \u0026#34;key not found\\n\u0026#34; Actual response: \u0026#34;\\n\u0026#34; Your server should return 404 Not Found when a key doesn\u0026#39;t exist. Check your key lookup logic and error handling. FAILED âœ— Read the guide: lsfr.io/kv-store/http-api You can also add your own logging to help debug. Your server\u0026rsquo;s output (stdout/stderr) is captured in node.log inside the working directory.\n"},{"id":3,"href":"/reference/testing/","title":"Testing","section":"Reference","content":"Testing Framework# The lsfr testing framework (attest) provides a fluent API for writing black-box tests against programs. Tests validate external behavior without accessing implementation internals.\nQuick Start# A typical test suite:\npackage kvstore import ( . \u0026#34;github.com/st3v3nmw/lsfr/internal/attest\u0026#34; ) func HTTPAPI() *Suite { return New(). // 0 Setup(func(do *Do) { do.Start(\u0026#34;node\u0026#34;) }). // 1 Test(\u0026#34;PUT stores data\u0026#34;, func(do *Do) { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key\u0026#34;, \u0026#34;value\u0026#34;).T(). Status(Is(200)). Assert(\u0026#34;Your server should accept PUT requests.\\n\u0026#34; + \u0026#34;Ensure your HTTP handler processes PUT to /kv/{key}.\u0026#34;) }). // 2 Test(\u0026#34;GET retrieves data\u0026#34;, func(do *Do) { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;).T(). Status(Is(200)). Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Your server should return stored values.\u0026#34;) }) }Tests run sequentially. State persists between tests so data written in test 1 is available in test 2. First failure stops execution.\nHTTP Assertions# Make HTTP requests and validate responses:\n// Basic request do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;).T(). Status(Is(200)). Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Your server should return stored values.\u0026#34;) // With body do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key\u0026#34;, \u0026#34;value\u0026#34;).T(). Status(Is(200)). Assert(\u0026#34;Your server should accept PUT requests.\u0026#34;) // With custom headers do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;POST\u0026#34;, \u0026#34;/api\u0026#34;, `{\u0026#34;key\u0026#34;:\u0026#34;value\u0026#34;}`, H{\u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34;}).T(). Status(Is(201)). Assert(...) // Status only do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;DELETE\u0026#34;, \u0026#34;/kv/key\u0026#34;).T(). Status(Is(200)). Assert(\u0026#34;Your server should accept DELETE requests.\u0026#34;)CLI Assertions# Execute CLI commands and validate output:\n// Check exit code and output do.Exec(\u0026#34;--help\u0026#34;).T(). ExitCode(Is(0)). Output(Contains(\u0026#34;Usage:\u0026#34;)). Assert(\u0026#34;Your command should show usage information.\u0026#34;) // Check exit code only do.Exec(\u0026#34;invalid\u0026#34;, \u0026#34;args\u0026#34;).T(). ExitCode(Is(1)). Assert(\u0026#34;Your command should reject invalid arguments.\u0026#34;)Matchers# Is(value)# Exact equality:\n.Status(Is(200)) .Body(Is(\u0026#34;key not found\\n\u0026#34;)) .ExitCode(Is(0))IsNull[T]()# Checks if a value is null:\n// Check for null JSON field do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/cluster/info\u0026#34;).T(). Status(Is(200)). JSON(\u0026#34;leader\u0026#34;, IsNull[string]()). Assert(\u0026#34;Leader should be null when no leader elected\u0026#34;)Requires a type parameter to specify the expected field type.\nContains(substring)# String contains:\n.Body(Contains(\u0026#34;error\u0026#34;)) .Output(Contains(\u0026#34;Usage:\u0026#34;))Matches(pattern)# Regex matching:\n.Body(Matches(`^[0-9]+$`)) .Output(Matches(`version \\d+\\.\\d+\\.\\d+`))OneOf(values...)# Match any of several values:\n// Useful for concurrent operations where order is non-deterministic .Body(OneOf(\u0026#34;value1\u0026#34;, \u0026#34;value2\u0026#34;, \u0026#34;value3\u0026#34;))Not(matcher)# Negates another matcher:\n.Status(Not(Is(500))) .Body(Not(Contains(\u0026#34;panic\u0026#34;)))JSON(path, matchers...)# Extract and validate JSON fields:\n// Simple field do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/cluster/info\u0026#34;).T(). Status(Is(200)). JSON(\u0026#34;role\u0026#34;, Is(\u0026#34;follower\u0026#34;)). JSON(\u0026#34;term\u0026#34;, Is(\u0026#34;1\u0026#34;)). Assert(\u0026#34;Should return cluster info\u0026#34;) // Nested path using dot notation do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/log\u0026#34;).T(). Status(Is(200)). JSON(\u0026#34;entries.0.term\u0026#34;, Is(\u0026#34;1\u0026#34;)). JSON(\u0026#34;entries.1.index\u0026#34;, Is(\u0026#34;1\u0026#34;)). Assert(\u0026#34;Should return log entries\u0026#34;) // Check for null do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/cluster/info\u0026#34;).T(). Status(Is(200)). JSON(\u0026#34;leader\u0026#34;, IsNull[string]()). Assert(\u0026#34;Leader should be null when no leader elected\u0026#34;) // Multiple matchers on same field do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/cluster/info\u0026#34;).T(). Status(Is(200)). JSON(\u0026#34;role\u0026#34;, Is(\u0026#34;leader\u0026#34;), Not(Is(\u0026#34;follower\u0026#34;)), Not(Is(\u0026#34;candidate\u0026#34;))). Assert(\u0026#34;Node should be leader\u0026#34;)Multiple Matchers# Chain multiple matchers for the same field:\n// Multiple status checks do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/\u0026#34;).T(). Status(Is(200), Not(Is(404)), Not(Is(500))). Assert(\u0026#34;Should return 200 OK\u0026#34;) // Multiple body checks do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/\u0026#34;).T(). Status(Is(200)). Body(Contains(\u0026#34;Hello\u0026#34;), Contains(\u0026#34;World\u0026#34;), Not(Contains(\u0026#34;Goodbye\u0026#34;))). Assert(\u0026#34;Should contain Hello and World but not Goodbye\u0026#34;)All matchers for a field must pass. If any matcher fails, the assertion fails.\nTiming# Eventually()# Retry until condition becomes true or timeout (default 5s):\n// Wait for replica to sync do.HTTP(\u0026#34;replica\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Eventually().T(). Status(Is(200)). Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Replica should eventually receive replicated data.\u0026#34;) // Custom timeout do.HTTP(\u0026#34;replica\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Eventually().Within(10 * time.Second).T(). Status(Is(200)). Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Replica should sync within 10 seconds.\u0026#34;)Consistently()# Verify condition stays true for duration (default 5s):\n// Verify value remains stable do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Consistently().T(). Status(Is(200)). Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Value should remain stable.\u0026#34;) // Custom duration do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;). Consistently().For(2 * time.Second).T(). Status(Is(200)). Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Value should remain stable for 2 seconds.\u0026#34;)Default (Immediate)# Without Eventually() or Consistently(), checks execute once immediately:\ndo.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;).T(). Status(Is(200)). Assert(\u0026#34;Your server should return the value immediately.\u0026#34;)Service Management# Start(name, args...)# Start a service with auto-assigned port:\ndo.Start(\u0026#34;node\u0026#34;) do.Start(\u0026#34;replica\u0026#34;, \u0026#34;--seed=123\u0026#34;)Port is auto-assigned by the OS. Services receive --port and --working-dir flags automatically. Each test run gets an isolated working directory like run-20240115-143022.\nStop(name)# Graceful shutdown with SIGTERM:\ndo.Stop(\u0026#34;node\u0026#34;)Sends SIGTERM and waits for graceful exit. If process doesn\u0026rsquo;t exit within timeout, sends SIGKILL.\nKill(name)# Immediate termination with SIGKILL:\ndo.Kill(\u0026#34;node\u0026#34;)Restart(name, sig...)# Restart a service:\n// Graceful restart (SIGTERM) do.Restart(\u0026#34;node\u0026#34;) // Crash simulation (SIGKILL) do.Restart(\u0026#34;node\u0026#34;, syscall.SIGKILL)The optional signal parameter controls how the process is stopped before restart. SIGKILL simulates crashes with no cleanup, SIGTERM allows graceful shutdown.\nConcurrency# Concurrently(funcs...)# Run operations in parallel:\ndo.Concurrently( func() { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key1\u0026#34;, \u0026#34;value1\u0026#34;).T(). Status(Is(200)). Assert(...) }, func() { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key2\u0026#34;, \u0026#34;value2\u0026#34;).T(). Status(Is(200)). Assert(...) }, ) // Verify both succeeded do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key1\u0026#34;).T(). Status(Is(200)). Body(Is(\u0026#34;value1\u0026#34;)). Assert(...) do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key2\u0026#34;).T(). Status(Is(200)). Body(Is(\u0026#34;value2\u0026#34;)). Assert(...)Waits for all functions to complete. If any panic, the first panic is re-raised after all complete.\nWriting Good Assertions# Assertion messages appear when tests fail. They should help developers fix the problem.\nGood assertion messages describe expected behavior, provide concrete next steps, and reference relevant concepts:\nAssert(\u0026#34;Your server should reject empty keys.\\n\u0026#34; + \u0026#34;Add validation to return 400 Bad Request for empty keys.\u0026#34;) Assert(\u0026#34;Your server should preserve data across crashes.\\n\u0026#34; + \u0026#34;Implement a Write-Ahead Log (WAL) that records operations before applying them.\\n\u0026#34; + \u0026#34;Ensure writes are durably stored before acknowledging to the client.\u0026#34;)Focus on requirements rather than implementation details. Say \u0026ldquo;Ensure writes are durably stored before acknowledging\u0026rdquo; instead of \u0026ldquo;You must call fsync\u0026rdquo;.\nAvoid:\nGeneric messages: \u0026ldquo;Fix your code\u0026rdquo; Vague messages: \u0026ldquo;This is wrong\u0026rdquo; Past tense: \u0026ldquo;Your server accepted an empty key when it should reject it\u0026rdquo; Unnecessary adverbs: \u0026ldquo;Your server should handle requests correctly\u0026rdquo; Example error output:\nPUT /kv/ \u0026#34;value\u0026#34; Expected response: \u0026#34;key cannot be empty\\n\u0026#34; Actual response: \u0026#34;\u0026#34; Your server should reject empty keys. Add validation to return 400 Bad Request for empty keys. Creating a Challenge# Directory Structure# challenges/ â””â”€â”€ kvstore/ â”œâ”€â”€ init.go # Challenge registration â”œâ”€â”€ http_api.go # Stage 1 â”œâ”€â”€ persistence.go # Stage 2 â””â”€â”€ crash_recovery.go # Stage 3Stage Structure# Each stage is a function returning *Suite:\npackage kvstore import ( . \u0026#34;github.com/st3v3nmw/lsfr/internal/attest\u0026#34; ) func HTTPAPI() *Suite { return New(). // 0 Setup(func(do *Do) { do.Start(\u0026#34;node\u0026#34;) }). // 1 Test(\u0026#34;PUT Basic Operations\u0026#34;, func(do *Do) { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;PUT\u0026#34;, \u0026#34;/kv/key\u0026#34;, \u0026#34;value\u0026#34;).T(). Status(Is(200)). Assert(\u0026#34;Your server should accept PUT requests.\u0026#34;) }). // 2 Test(\u0026#34;GET Basic Operations\u0026#34;, func(do *Do) { do.HTTP(\u0026#34;node\u0026#34;, \u0026#34;GET\u0026#34;, \u0026#34;/kv/key\u0026#34;).T(). Status(Is(200)). Body(Is(\u0026#34;value\u0026#34;)). Assert(\u0026#34;Your server should return stored values.\u0026#34;) }) }Import attest with . for cleaner syntax. Number tests with comments (0, 1, 2\u0026hellip;) to visually separate tests so that they\u0026rsquo;re not crowded.\nChallenge Registration# Create init.go:\npackage kvstore import \u0026#34;github.com/st3v3nmw/lsfr/internal/registry\u0026#34; func init() { challenge := \u0026amp;registry.Challenge{ Name: \u0026#34;Distributed Key-Value Store\u0026#34;, Summary: `Build a distributed key-value store from scratch. You\u0026#39;ll start with a simple HTTP API and progressively add persistence, crash recovery, clustering, replication, and consensus.`, } challenge.AddStage(\u0026#34;http-api\u0026#34;, \u0026#34;Store and Retrieve Data\u0026#34;, HTTPAPI) challenge.AddStage(\u0026#34;persistence\u0026#34;, \u0026#34;Data Survives SIGTERM\u0026#34;, Persistence) challenge.AddStage(\u0026#34;crash-recovery\u0026#34;, \u0026#34;Data Survives SIGKILL\u0026#34;, CrashRecovery) challenge.AddStage(\u0026#34;leader-election\u0026#34;, \u0026#34;Cluster Elects and Maintains Leader\u0026#34;, LeaderElection) registry.RegisterChallenge(\u0026#34;kv-store\u0026#34;, challenge) }Auto-Discovery# Import your challenge in challenges/challenges.go:\npackage challenges import ( _ \u0026#34;github.com/st3v3nmw/lsfr/challenges/kvstore\u0026#34; )"},{"id":4,"href":"/guides/ci-cd/","title":"CI/CD","section":"Guides","content":"CI/CD# Run lsfr tests automatically in GitHub Actions.\nGitHub Actions# Add .github/workflows/lsfr.yaml to your repository:\nname: lsfr Tests on: push: branches: [main] pull_request: branches: [main] jobs: test: runs-on: ubuntu-latest steps: - uses: actions/checkout@v6 - uses: st3v3nmw/lsfr-action@mainThe action runs lsfr test on every push to main and on pull requests.\nCustom Working Directory# If your lsfr.yaml isn\u0026rsquo;t at the repository root:\n- uses: st3v3nmw/lsfr-action@main with: working-directory: \u0026#39;./my-challenge\u0026#39;"},{"id":5,"href":"/kv-store/","title":"Distributed Key-Value Store","section":"Home","content":"Distributed Key-Value Store Challenge# Welcome to the distributed key-value store challenge!\nIn this challenge, you\u0026rsquo;ll build a distributed key-value store from scratch. You\u0026rsquo;ll start with a single-node system that handles persistence and crash recovery, then implement Raft\u0026rsquo;s leader election, log replication, and fault tolerance mechanisms.\nBy the end, you\u0026rsquo;ll have built a production-grade distributed system that maintains strong consistency guarantees even during node failures and network partitions.\nStages# HTTP API Build a basic in-memory key-value store with GET/PUT/DELETE operations over HTTP.\nPersistence Add persistence to your store. Data should survive clean shutdowns (SIGTERM).\nCrash Recovery Ensure data consistency after crashes. Data should survive unclean shutdowns (SIGKILL).\nLeader Election Form a cluster and elect a leader using the Raft consensus algorithm.\nLog Replication Replicate operations from the leader to followers with strong consistency guarantees.\nMembership Changes Dynamically add and remove nodes from the cluster without downtime.\nFault Tolerance Handle node failures and network partitions while maintaining safety guarantees.\nLog Compaction Prevent unbounded log growth through snapshots and log truncation.\nGetting Started# If you haven\u0026rsquo;t already, read this overview on how lsfr works and then start with stage 1 (HTTP API).\nResources# Books# Designing Data-Intensive Applications by Martin Kleppmann Database Internals by Alex Petrov Papers# The Raft Paper by Diego Ongaro \u0026amp; John Ousterhout Videos# Distributed Systems lecture series by Martin Kleppmann Implementations# little-key-value in Go by @st3v3nmw "},{"id":6,"href":"/kv-store/persistence/","title":"Persistence","section":"Distributed Key-Value Store","content":"Persistence# In this stage, you\u0026rsquo;ll add persistence to your key-value store. Data should survive clean shutdowns and be restored when the server restarts.\nGraceful Shutdown# When your server receives a SIGTERM signal, it should:\nWait for in-flight requests to complete (within 5 seconds) Save all key-value pairs to disk Exit with status code 0 Startup Recovery# When your server starts, it should:\nCheck the working directory for existing data Load any previously saved key-value pairs Continue serving requests with the restored data If no previous data exists, start with an empty store.\nStorage# Save your in-memory state to disk during shutdown and restore it on startup. Create your data files in the working directory (passed via --working-dir). The serialization format and file naming are up to you - JSON, binary, plain text, whatever.\nThis approach survives clean shutdowns but not crashes. If the process dies unexpectedly, you\u0026rsquo;ll lose any data that wasn\u0026rsquo;t saved. That\u0026rsquo;s fine for this stage - you\u0026rsquo;ll add crash recovery in the next stage.\nTesting# Your server will be started with the working directory where it should store data:\n$ ./run.sh --port 8001 --working-dir .lsfr/run-20251226-210357 You can test your implementation using the lsfr command:\n$ lsfr test Testing persistence: Data Survives SIGTERM âœ“ Verify Data Survives Graceful Restart âœ“ Check Data Integrity After Multiple Restarts âœ“ Test Persistence When Under Concurrent Load PASSED âœ“ Run \u0026#39;lsfr next\u0026#39; to advance to the next stage. The tests will:\nStore data in your server Send SIGTERM to trigger graceful shutdown Restart your server Verify all data is still present "},{"id":7,"href":"/reference/writing-style/","title":"Writing Style","section":"Reference","content":"Writing Style# Audience# Write for capable peers: mid-level+ developers who can research concepts, make implementation choices, and work on complex problems.\nState what the system needs to accomplish, not how to build it. Point to possible approaches or considerations, but let readers make implementation choices.\nPedagogy# Structure learning through progressive disclosure. Start each challenge with the simplest version of a problem, then add complexity only after the foundation is solid. Make each stage build on previous work so the progression feels natural rather than arbitrary.\nIntroduce one major capability per stage: one testable system behavior that builds on previous work. Supporting concepts can appear as needed, but each stage should add one clear building block to the system. Describe stages based on the capability they add, not the mechanism used to achieve it. This keeps focus on what the system accomplishes at each stage.\nConstrain interfaces and contracts that affect testing or compatibility. Leave internal implementation details (data structures, algorithms, optimizations) to the reader\u0026rsquo;s judgment.\nLink to external resources when you introduce concepts. Point to tutorials, papers, lectures, or reference implementations - whatever explains it best. Don\u0026rsquo;t replicate explanations that already exist; point to the best resource and move on.\nVoice \u0026amp; Tone# Address the reader directly using second person (\u0026ldquo;you\u0026rdquo;) and active voice. Write as if you\u0026rsquo;re giving clear directions to a colleague, not lecturing from a podium. Use simple, everyday language.\nGet to the point immediately. Skip introductory context about why topics are important - your reader already has that context. Brief section transitions are fine, but avoid elaborate setup. Do explain the reasoning behind specific constraints or design choices you\u0026rsquo;re imposing.\nMaintain a matter-of-fact tone. Skip reassurance and cheerleading. An occasional emoji or light moment is fine, but default to straightforward instruction.\nFormatting# Structure each stage with a brief introduction (one or two sentences on what the reader will build), followed by precise specifications, implementation guidance, and testing instructions. Optionally include a debugging section with example test failures or debugging techniques. When showing failures, provide actionable guidance: expected versus actual output, then what to check or what might have gone wrong.\nWhen defining API contracts, data formats, or expected behaviors, be exact. Include complete endpoint specifications with methods, parameters, responses, and literal error messages. Specify data constraints explicitly. The goal is removing ambiguity about what to build without prescribing how to build it.\nUse code blocks to show test invocations, command-line usage, and expected outputs - not implementation code or algorithms. Readers should see how to verify their work and what correct behavior looks like. When test behavior isn\u0026rsquo;t obvious from output alone, briefly explain what the test does, especially when it affects implementation constraints (which signals, timing requirements, error conditions to handle).\nReserve callouts for critical non-obvious concerns. Don\u0026rsquo;t use them for general information that belongs in body text.\nLink concepts inline when they first appear. Add comprehensive resources like books, lecture series, and reference implementations to the challenge\u0026rsquo;s index page.\n"},{"id":8,"href":"/kv-store/crash-recovery/","title":"Crash Recovery","section":"Distributed Key-Value Store","content":"Crash Recovery# Your server currently saves data on clean shutdown but loses everything if it crashes. In this stage, you\u0026rsquo;ll add durability so data survives unexpected failures.\nWrite-Ahead Logging# Implement a Write-Ahead Log (WAL) that records operations before they\u0026rsquo;re applied to memory. Each write operation must be written to the log file before updating your in-memory store.\nLog Format# Your log should record operations in append-only fashion. The format is up to you - JSONL (one JSON object per line), binary serialization, or plain text all work.\nEach log entry needs enough information to replay the operation:\nOperation type (e.g., \u0026ldquo;set\u0026rdquo;, \u0026ldquo;delete\u0026rdquo;, \u0026ldquo;clear\u0026rdquo;) Key Value Any other metadata you need for replay Durability# After appending an operation to the log, ensure it\u0026rsquo;s physically written to disk before responding to the client. Use your language\u0026rsquo;s file sync mechanism (fsync, flush, etc.) to force the operating system to persist the write.\nWithout sync, the OS may buffer writes in memory and you\u0026rsquo;ll lose data on crash.\nSyncing on every operation is slow since you\u0026rsquo;re forcing a disk write and blocking the response. This is the correct trade-off for durability, but it limits throughput. Production databases use techniques like batching to amortize the fsync cost across multiple operations.\nRecovery Procedure# When your server starts:\nLoad the most recent snapshot (from the persistence stage) if one exists Replay all operations from the WAL that occurred after the snapshot Resume serving requests If no snapshot exists, replay the entire log from the beginning.\nCheckpointing# As your log grows, replaying from the beginning becomes slow. Periodically create snapshots of your in-memory state and truncate the log.\nWhen to checkpoint is up to you - after N operations, every M seconds, when the log reaches a certain size, etc. The test doesn\u0026rsquo;t care about your checkpoint strategy, only that recovery works correctly.\nAfter creating a snapshot:\nWrite the snapshot to a new file Truncate or create a new WAL file Continue logging operations On recovery, load the latest snapshot and replay only the operations logged after that snapshot.\nStorage Layout# You now have two types of files:\nSnapshot: Full state at a point in time (from previous stage) WAL: Operations logged since the last snapshot Organize these in the working directory however makes sense - separate files, subdirectories, naming conventions, etc. The test only cares that recovery works, not how you structure the files.\nTesting# Your server will be started with the working directory:\n$ ./run.sh --port 8001 --working-dir .lsfr/run-20251226-210357 Your server will be tested with unexpected crashes:\n$ lsfr test crash-recovery Testing crash-recovery: Data Survives SIGKILL âœ“ Basic WAL Durability âœ“ Multiple Crash Recovery Cycles âœ“ Rapid Write Burst Before Crash âœ“ Test Recovery When Under Concurrent Load PASSED âœ“ Run \u0026#39;lsfr next\u0026#39; to advance to the next stage. The tests will:\nStore data in your server Kill the server process (SIGKILL) without warning Restart your server Verify all data that was acknowledged before the crash is still present "},{"id":9,"href":"/kv-store/leader-election/","title":"Leader Election","section":"Distributed Key-Value Store","content":"Leader Election# Implement Raft leader election to form a static 5-node cluster that elects a single leader.\nCluster Formation# Start nodes with the --peers flag specifying all cluster members:\n$ ./run.sh --port 8001 --working-dir .lsfr/run-T --peers=:8001,:8002,:8003,:8004,:8005 The --peers list includes all cluster members, including this node itself, and as such, all nodes receive the same --peers list.\nThe cluster is static; membership doesn\u0026rsquo;t change in this stage.\nLeader Election# All nodes start as followers. If a follower doesn\u0026rsquo;t receive heartbeats within the election timeout (randomized between 500-1,000ms), it becomes a candidate and starts an election.\nCandidates request votes from other nodes. A candidate becomes leader if it receives votes from a majority ($\\lceil (n+1)/2 \\rceil$ where $n$ is cluster size; 3 votes for a 5-node cluster). Each node grants at most one vote per term.\nTerms act as a logical clock. When a node discovers a higher term, it immediately updates its term and reverts to follower.\nHeartbeats# Leaders send AppendEntries RPC heartbeats every 100ms to maintain authority. The entries array is empty in this stage.\nIf a follower doesn\u0026rsquo;t receive heartbeats within the election timeout, it starts a new election.\nThese timing values are more conservative than the Raft paper\u0026rsquo;s suggested 150-300ms election timeout. The larger ratio (5-10x) between heartbeat interval and election timeout accounts for varying execution speeds across different implementation languages and prevents spurious elections caused by garbage collection pauses or interpreter overhead.\nClient Requests# Leaders accept GET/PUT/DELETE requests from the HTTP API stage.\nFollowers respond with 307 Temporary Redirect and Location header pointing to the current leader:\nHTTP/1.1 307 Temporary Redirect Location: http://127.0.0.1:8001/kv/mykeyIf no leader is known, return 503 Service Unavailable.\nPersistence# Persist currentTerm and votedFor to disk before responding to RequestVote RPC. Use fsync to ensure durability.\nState must survive crashes and restarts. A node that crashes and restarts should resume with its persisted term and vote.\nAPI# Raft RPCs# POST /raft/request-vote RequestVote RPC - invoked by candidates to gather votes during leader election.\nPOST /raft/request-vote Body: { \u0026#34;term\u0026#34;: \u0026lt;int\u0026gt;, # candidate\u0026#39;s term \u0026#34;candidate-id\u0026#34;: \u0026#34;:\u0026lt;port\u0026gt;\u0026#34;, # candidate requesting vote \u0026#34;last-log-index\u0026#34;: \u0026lt;int\u0026gt;, # index of candidate\u0026#39;s last log entry \u0026#34;last-log-term\u0026#34;: \u0026lt;int\u0026gt; # term of candidate\u0026#39;s last log entry } Response: - 200 OK: { \u0026#34;term\u0026#34;: \u0026lt;int\u0026gt;, # current-term, for candidate to update itself \u0026#34;vote-granted\u0026#34;: \u0026lt;bool\u0026gt; # true means candidate received vote }In this stage with no log entries yet, set last-log-index and last-log-term to 0.\nPOST /raft/append-entries AppendEntries RPC - used for heartbeats (empty entries) to maintain leader authority.\nPOST /raft/append-entries Body: { \u0026#34;term\u0026#34;: \u0026lt;int\u0026gt;, # leader\u0026#39;s term \u0026#34;leader-id\u0026#34;: \u0026#34;:\u0026lt;port\u0026gt;\u0026#34;, # so follower can redirect clients \u0026#34;prev-log-index\u0026#34;: \u0026lt;int\u0026gt;, # index of log entry immediately preceding new ones \u0026#34;prev-log-term\u0026#34;: \u0026lt;int\u0026gt;, # term of prev-log-index entry \u0026#34;entries\u0026#34;: [], # empty for heartbeats; will contain log entries later \u0026#34;leader-commit\u0026#34;: \u0026lt;int\u0026gt; # leader\u0026#39;s commit-index } Response: - 200 OK: { \u0026#34;term\u0026#34;: \u0026lt;int\u0026gt;, # current-term, for leader to update itself \u0026#34;success\u0026#34;: \u0026lt;bool\u0026gt; # true if follower contained entry matching prev-log-index/prev-log-term }In this stage with no log entries yet, set prev-log-index and prev-log-term to 0.\nTesting APIs# The endpoints are lsfr-specific for testing and observability.\nGET /cluster/info Get the node\u0026rsquo;s current cluster state and role.\nGET /cluster/info Response: - 200 OK: { \u0026#34;role\u0026#34;: \u0026#34;leader|candidate|follower\u0026#34;, \u0026#34;term\u0026#34;: \u0026lt;int\u0026gt;, \u0026#34;leader\u0026#34;: \u0026#34;:\u0026lt;port\u0026gt;\u0026#34;, # null if no known leader \u0026#34;voted-for\u0026#34;: \u0026#34;:\u0026lt;port\u0026gt;\u0026#34;, # null if haven\u0026#39;t voted this term # own address if voted for self \u0026#34;peers\u0026#34;: [\u0026#34;:\u0026lt;port\u0026gt;\u0026#34;, ...] # all cluster members, sorted lexicographically }Returns a snapshot of the node\u0026rsquo;s state at the instant the request is made. The leader field is null when no leader is known (startup, during election, after leader failure).\nPOST /cluster/partition Simulate a network partition by blocking Raft RPC communication with specific peers.\nPOST /cluster/partition Body: { \u0026#34;peers\u0026#34;: [\u0026#34;:\u0026lt;port\u0026gt;\u0026#34;, \u0026#34;:\u0026lt;port\u0026gt;\u0026#34;] # nodes this node can communicate with # empty list = isolated from all nodes } Response: - 200 OK: Partition applied successfullyBidirectionally blocks /raft/* RPC endpoints between this node and all nodes NOT in the peers list. Takes effect immediately; in-flight RPCs complete. /kv/* client requests and /cluster/* testing end points are not affected.\nPartition state persists across crashes and restarts. A node that crashes and restarts while partitioned restarts with the partition still active.\nPOST /cluster/heal Restore full network connectivity, removing any active partitions.\nPOST /cluster/heal Response: - 200 OK: Network connectivity restoredRemoves all partition restrictions immediately. Idempotent (safe to call when not partitioned). Heal state persists across crashes and restarts.\nTesting# Your server will be started as a 5-node cluster:\n$ ./run.sh --port 8001 --working-dir .lsfr/run-T --peers=:8001,:8002,:8003,:8004,:8005 The tests will verify leader election behavior:\n$ lsfr test leader-election Testing leader-election: Cluster Elects and Maintains Leader âœ“ Leader Election Completes âœ“ Exactly One Leader Per Term âœ“ Leader Maintains Authority via Heartbeats âœ“ Follower Redirects Clients to Leader âœ“ State Survives Crashes âœ“ Minority Partition Cannot Elect Leader âœ“ Majority Partition Elects Leader âœ“ Healing After Partition PASSED âœ“ Run \u0026#39;lsfr next\u0026#39; to advance to the next stage. Example failure:\n$ lsfr test Testing leader-election: Cluster Elects and Maintains Leader âœ“ Leader Election Completes âœ“ Exactly One Leader Per Term âœ“ Leader Maintains Authority via Heartbeats âœ“ Follower Redirects Clients to Leader âœ“ State Survives Crashes âœ— Minority Partition Cannot Elect Leader GET http://127.0.0.1:8000/cluster/info Expected: No leader elected in minority partition (2 nodes) after 5 seconds Actual: Node :8001 became leader in minority partition Are you requiring majority votes? FAILED âœ— Read the guide: lsfr.io/kv-store/leader-election Resources# Raft Visualization by The Secret Lives of Data The Raft Consensus Algorithm Distributed Systems 6.2: Raft by Martin Kleppmann Students\u0026rsquo; Guide to Raft by Jon Gjengset "},{"id":10,"href":"/kv-store/log-replication/","title":"Log Replication","section":"Distributed Key-Value Store","content":"Log Replication# // TODO\nRough Notes# Raft log replaces the WAL from stage 3 Leader appends client operations to log, replicates to followers AppendEntries RPC with actual log entries Commit index (entry committed when replicated to majority) Apply committed entries to state machine (KV store) Log matching property (consistency across nodes) Fsync log entries before responding to client nextIndex and matchIndex tracking per follower Happy path only - no crashes/failures yet "},{"id":11,"href":"/kv-store/membership-changes/","title":"Membership Changes","section":"Distributed Key-Value Store","content":"Membership Changes# // TODO\nRough Notes# Dynamically add/remove nodes from cluster Replicate configuration changes through Raft log Joint consensus (C_old,new) to prevent split brain during transitions Or single-server changes (simpler, safer) AddServer and RemoveServer operations Catch up new servers before adding to cluster Handle leader stepping down if removed from cluster Cluster Management API# POST /cluster/join Join a cluster.\nPOST /cluster/join Body: { \u0026#34;leader\u0026#34;: \u0026#34;:\u0026lt;port\u0026gt;\u0026#34; # the leader\u0026#39;s address } Response: - 200 OK: Successfully joined cluster POST /cluster/leave Leave the cluster.\nPOST /cluster/leave Response: - 200 OK: Successfully initiated removal from cluster"},{"id":12,"href":"/kv-store/fault-tolerance/","title":"Fault Tolerance","section":"Distributed Key-Value Store","content":"Fault Tolerance# // TODO\nRough Notes# Handle leader crashes (followers detect timeout, elect new leader) Handle follower crashes (leader retries AppendEntries) Log repair on recovery (conflicting entries, missing entries) Network partitions (majority partition continues, minority blocks) Safety: committed entries never lost Log consistency checks (prevLogIndex, prevLogTerm) Truncate conflicting uncommitted entries Leader completeness property "},{"id":13,"href":"/kv-store/log-compaction/","title":"Log Compaction","section":"Distributed Key-Value Store","content":"Log Compaction# // TODO\nRough Notes# Prevent unbounded log growth Take snapshots of state machine (KV store) Truncate log after snapshot InstallSnapshot RPC for slow/catching-up followers Snapshot metadata (last included index, last included term) Incremental snapshots or full state dump When to snapshot (configurable: every N entries, every M seconds, size threshold) Note: snapshot size = data size problem (document limitation, point to storage engine solutions) "},{"id":14,"href":"/about/","title":"About","section":"Home","content":"About# lsfr /ËˆÉ›l ËˆÉ›s ËˆÉ›f ËˆÉ‘r/ noun\nNamed after the song \u0026ldquo;Love Songs For Robots\u0026rdquo; by Patrick Watson.\nI have always been interested in distributed systems so when I finally got around to reading Designing Data-Intensive Applications, I thought \u0026ldquo;you know what, I should actually build some of this stuff to really understand it\u0026rdquo;. And that\u0026rsquo;s how lsfr was born: it breaks down complex systems into stages you can implement and test.\nBuilt by Stephen Mwangi.\n"}]